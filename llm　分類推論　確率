import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import AutoModel, AutoTokenizer


import pandas as pd
import torch
from transformers import BertForSequenceClassification
import transformers
from transformers import AdamW
from torch.nn import CrossEntropyLoss

from datasets import load_dataset
import pandas as pd
from transformers import DebertaTokenizer, DebertaForSequenceClassification
from torch.utils.data import DataLoader
import torch
import os
from scipy.special import softmax
from sklearn.metrics import roc_auc_score
os.environ['WANDB_DISABLED']='true'
from transformers import AutoModelForSequenceClassification

test_df = pd.read_csv("/kaggle/input/llm-detect-ai-generated-text/test_essays.csv")
sample= pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')

from datasets import Dataset

test_dataset = Dataset.from_pandas(test_df)

from datasets import DatasetDict
dataset2 = DatasetDict({"test": test_dataset
    
})

tokenizer = AutoTokenizer\
    .from_pretrained("/kaggle/input/tokenizer-llm-detect-ai-generated")

model = (AutoModelForSequenceClassification
    .from_pretrained("/kaggle/input/debert-llm-detect-ai"))#.to(device))
def tokenize2(batch):
    enc =  tokenizer(batch["text"], padding=True, truncation=True)
    return enc
dataset_encoded2 = dataset2.map(tokenize2)
small_test_dataset = dataset_encoded2['test']
tokenizer = AutoTokenizer\
    .from_pretrained("/kaggle/input/tokenizer-llm-detect-ai-generated")

model = (AutoModelForSequenceClassification
    .from_pretrained("/kaggle/input/debert-llm-detect-ai"))#.to(device))

def compute_roc_auc(eval_pred):
    logits, labels = eval_pred.predictions, eval_pred.label_ids
    if labels.std() < 1E-8: # only one class present in dataset
        return {"roc_auc": 0.0}
    ps = softmax(logits, axis=-1)[:,1]
    return {"roc_auc": roc_auc_score(labels, ps)}
from transformers import TrainingArguments
#batch_size = 16でダメ
batch_size = 4
model_name = "/kaggle/working/"

training_args = TrainingArguments(
    output_dir=model_name,
    num_train_epochs=2,
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    disable_tqdm=False,
    #logging_steps=logging_steps,
    push_to_hub=False,
    log_level="error", 
    gradient_accumulation_steps=8,
    metric_for_best_model="roc_auc")
    
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    tokenizer=tokenizer,
    compute_metrics=compute_roc_auc,
)
#trainer.train()
final_preds = trainer.predict(small_test_dataset)#(dataset2['test'])[:,1]
logits = final_preds.predictions
print(logits.shape)
print(logits[:,0])
probs = (np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True))[:,1]
sample['generated'] =probs #final_preds
sample.to_csv('submission.csv', index=False)
sample.head(n=None)
